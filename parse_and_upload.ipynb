{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Stats Can data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of files to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = 'dissemination_area'\n",
    "files = [f'{files_dir}/' + f for f in os.listdir(files_dir) if f.startswith('98-401-X2016044')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to extract region from file name\n",
    "file_re = re.compile(\"98-401-X2016044_(.+)_English_CSV_data\\.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dissemination_area/98-401-X2016044_ONTARIO_English_CSV_data.csv',\n",
       " 'dissemination_area/98-401-X2016044_QUEBEC_English_CSV_data.csv',\n",
       " 'dissemination_area/98-401-X2016044_ATLANTIC_English_CSV_data.csv',\n",
       " 'dissemination_area/98-401-X2016044_TERRITORIES_English_CSV_data.csv',\n",
       " 'dissemination_area/98-401-X2016044_BRITISH_COLUMBIA_English_CSV_data.csv',\n",
       " 'dissemination_area/98-401-X2016044_PRAIRIES_English_CSV_data.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions for parsing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean input CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_csv(file, geo_level=4):\n",
    "    df = pd.read_csv(file,\n",
    "                 dtype={\n",
    "                     'GEO_CODE (POR)': 'int64',\n",
    "                     'GEO_LEVEL': 'int64',\n",
    "                     'DIM: Profile of Dissemination Areas (2247)': 'str',\n",
    "                     'Member ID: Profile of Dissemination Areas (2247)': 'int64',\n",
    "                     'Dim: Sex (3): Member ID: [1]: Total - Sex': 'str',\n",
    "                     'Dim: Sex (3): Member ID: [2]: Male': 'str',\n",
    "                     'Dim: Sex (3): Member ID: [3]: Female': 'str'\n",
    "                 },\n",
    "                 usecols=[\n",
    "                    'GEO_CODE (POR)',\n",
    "                    'GEO_LEVEL',\n",
    "                    'DIM: Profile of Dissemination Areas (2247)',\n",
    "                    'Member ID: Profile of Dissemination Areas (2247)',\n",
    "                    'Dim: Sex (3): Member ID: [1]: Total - Sex',\n",
    "                    'Dim: Sex (3): Member ID: [2]: Male',\n",
    "                    'Dim: Sex (3): Member ID: [3]: Female'\n",
    "                ])\n",
    "    \n",
    "    df = df.loc[df.GEO_LEVEL == geo_level, \n",
    "                ['GEO_CODE (POR)',\n",
    "                 'DIM: Profile of Dissemination Areas (2247)',\n",
    "                 'Member ID: Profile of Dissemination Areas (2247)',\n",
    "                 'Dim: Sex (3): Member ID: [1]: Total - Sex',\n",
    "                 'Dim: Sex (3): Member ID: [2]: Male',\n",
    "                 'Dim: Sex (3): Member ID: [3]: Female'\n",
    "                ]]\n",
    "    \n",
    "    # Rename columns for easier access\n",
    "    df.columns = ['da', 'feature_desc', 'feature_num', 'total', 'male', 'female']\n",
    "\n",
    "    #Set columns to numeric\n",
    "    for c in ['total', 'male', 'female']:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract relavant rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_features(g,\n",
    "                   feature_nums, \n",
    "                   name, \n",
    "                   alt_feature_names=None, \n",
    "                   agg=False):\n",
    "    df = g[g.feature_num.isin(feature_nums)]\n",
    "    if alt_feature_names:\n",
    "        df['feature_desc'] = alt_feature_names\n",
    "    df = pd.melt(df, id_vars=['da', 'feature_desc', 'feature_num'], var_name='sex', value_name='count')\n",
    "    df = df.dropna()\n",
    "    df['feature'] = name\n",
    "    \n",
    "    if agg:\n",
    "        df = df.assign(pct=df['count'] / df.groupby('sex')['count'].transform('sum'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User Level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_user_level_features(g):\n",
    "    \n",
    "    parsed_dict = {\n",
    "        # individual stats\n",
    "        'age': parse_features(g, [10,11,12,14,15,16,17,18,19,20,21,22,23,25,26,27,28,30,31,32,33], 'age', agg=True),\n",
    "        \n",
    "        'martial_status': parse_features(g, [61, 62, 64, 65, 66, 67], 'martial status', alt_feature_names=['Married', 'Common law', 'Never married', 'Separated', 'Divorced', 'Widowed'], agg=True),\n",
    "\n",
    "        'official_lang': parse_features(g, [101, 102, 103, 104], 'knowledge of official language', agg=True),\n",
    "\n",
    "        'mother_tongue': parse_features(g, [115, 116, 117, 377, 378, 379, 380], 'mother tongue', agg=True),\n",
    "\n",
    "        'home_language': parse_features(g, [384, 385, 386, 646, 647, 648, 649], 'language spoken at home', agg=True),\n",
    "\n",
    "        'ttl_income': parse_features(g, [695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 706, 707], name='total income', agg=True),\n",
    "\n",
    "        'employment_income': parse_features(g, [728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 739, 740], name='employment income', agg=True),\n",
    "\n",
    "        'low_income': parse_features(g, [853, 855, 856], name='low income', agg=True),\n",
    "\n",
    "        'citizenship': parse_features(g, [1136,1139], name='citizenship', agg=True),\n",
    "\n",
    "        'immigrants': parse_features(g, [1141, 1142, 1150], name='immigrants', agg=True),\n",
    "\n",
    "        'minority': parse_features(g, [i for i in np.arange(1325, 1338)], name='visible minority', agg=True),\n",
    "\n",
    "        'education': parse_features(g, [1684, 1685, 1687, 1690, 1691, 1693, 1694, 1695, 1696, 1697], name='education', agg=True),\n",
    "\n",
    "        'employment': parse_features(g, [1867, 1868, 1867, 1869], name='employment', agg=True),\n",
    "        \n",
    "        \n",
    "        # household stats\n",
    "        'dwelling': parse_features(g, [42, 43, 44, 50], name='dwelling', agg=True),\n",
    "\n",
    "        'household_size': parse_features(g, [52, 53, 54, 55, 56], 'household size', agg=True),\n",
    "\n",
    "        'household_income': parse_features(g, np.concatenate([np.arange(760, 775), np.arange(776, 780)]), name='household income', agg=True),\n",
    "\n",
    "        'family': parse_features(g, [94, 95, 98], name='family', agg=True, alt_feature_names=['Family without children', 'Family with children', 'One-person household']),\n",
    "        \n",
    "        # Individual, but each individual can be in more than 1 category\n",
    "        'ethnicorigin': parse_features(g, [1339, 1343, 1353, 1427, 1448, 1473, 1541, 1607], name='ethnic origin', agg=True)\n",
    "    }\n",
    "\n",
    "    final_df = pd.concat(list(parsed_dict.values()))\n",
    "    return final_df[['da', 'feature', 'feature_num', 'feature_desc', 'sex', 'count', 'pct']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_overall_features(g):\n",
    "    \n",
    "    parsed_dict = {\n",
    "         'overall_stats': parse_features(g, [1, 6, 7, 58, 73], 'overall stats'),\n",
    "         'age': parse_features(g, [39, 40], 'age', alt_feature_names= ['Average', 'Median']),\n",
    "         'indiv_income': parse_features(g, [663, 671, 674, 682], name='individual income', \n",
    "               alt_feature_names=['Median total income (2015)', 'Median employment income (2015)', 'Average total income (2015)', 'Average employment income']),\n",
    "         'household_income': parse_features(g, [742, 745, 748, 751, 754, 757], name='household income',\n",
    "              alt_feature_names=['Median total income', 'Median total income of one-person households', 'Median total income of two-or-more households',\n",
    "                                 'Average total income', 'Average total income of one-person households', 'Average total income of two-or-more households']),\n",
    "        'dwelling_value': parse_features(g, [1676, 1677], name='dwelling value', alt_feature_names=['Median', 'Average']),\n",
    "        'dwelling_rent': parse_features(g, [1681, 1682], name='dwelling rent', alt_feature_names=['Median', 'Average']),\n",
    "        'employment_rate': parse_features(g, [1870, 1871, 1872], name='employment_rate')\n",
    "    }\n",
    "\n",
    "    final_df = pd.concat(list(parsed_dict.values()))\n",
    "    return final_df[['da', 'feature', 'feature_num', 'feature_desc', 'sex', 'count']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean dataframes first before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dissemination_area/98-401-X2016044_ONTARIO_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_QUEBEC_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_ATLANTIC_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_TERRITORIES_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_BRITISH_COLUMBIA_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_PRAIRIES_English_CSV_data.csv\n"
     ]
    }
   ],
   "source": [
    "# dissemination area\n",
    "for file in files:\n",
    "    print(f\"processing {file}\")\n",
    "    region = file_re.findall(file)[0]\n",
    "    df = read_and_clean_csv(file, geo_level=4)\n",
    "    df.to_parquet(f\"{files_dir}/raw_{region}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dissemination_area/98-401-X2016044_ONTARIO_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_QUEBEC_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_ATLANTIC_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_TERRITORIES_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_BRITISH_COLUMBIA_English_CSV_data.csv\n",
      "processing dissemination_area/98-401-X2016044_PRAIRIES_English_CSV_data.csv\n"
     ]
    }
   ],
   "source": [
    "# province\n",
    "for file in files:\n",
    "    print(f\"processing {file}\")\n",
    "    region = file_re.findall(file)[0]\n",
    "    df = read_and_clean_csv(file, geo_level=1)\n",
    "    df.to_parquet(f\"{files_dir}/raw_prov_{region}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Dask for multicore processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_PRAIRIES.parquet',\n",
       " 'raw_ATLANTIC.parquet',\n",
       " 'raw_TERRITORIES.parquet',\n",
       " 'raw_ONTARIO.parquet',\n",
       " 'raw_QUEBEC.parquet',\n",
       " 'raw_BRITISH_COLUMBIA.parquet']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files = [f for f in os.listdir(files_dir) if f.startswith('raw') and not f.startswith('raw_prov')]\n",
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing raw_PRAIRIES.parquet...\n",
      "- reading in parquet file\n",
      "Took 14.08 seconds\n",
      "- processing overall stats\n",
      "Took 377.27 seconds\n",
      "Done! raw_PRAIRIES.parquet took 391.36 seconds\n",
      "processing raw_ATLANTIC.parquet...\n",
      "- reading in parquet file\n",
      "Took 3.84 seconds\n",
      "- processing overall stats\n",
      "Took 161.76 seconds\n",
      "Done! raw_ATLANTIC.parquet took 165.60 seconds\n",
      "processing raw_TERRITORIES.parquet...\n",
      "- reading in parquet file\n",
      "Took 0.22 seconds\n",
      "- processing overall stats\n",
      "Took 10.22 seconds\n",
      "Done! raw_TERRITORIES.parquet took 10.44 seconds\n",
      "processing raw_ONTARIO.parquet...\n",
      "- reading in parquet file\n",
      "Took 13.99 seconds\n",
      "- processing overall stats\n",
      "Took 749.94 seconds\n",
      "Done! raw_ONTARIO.parquet took 763.95 seconds\n",
      "processing raw_QUEBEC.parquet...\n",
      "- reading in parquet file\n",
      "Took 10.83 seconds\n",
      "- processing overall stats\n",
      "Took 493.63 seconds\n",
      "Done! raw_QUEBEC.parquet took 504.48 seconds\n",
      "processing raw_BRITISH_COLUMBIA.parquet...\n",
      "- reading in parquet file\n",
      "Took 7.11 seconds\n",
      "- processing overall stats\n",
      "Took 278.29 seconds\n",
      "Done! raw_BRITISH_COLUMBIA.parquet took 285.42 seconds\n"
     ]
    }
   ],
   "source": [
    "with dask.config.set({'temporary_directory': '/home/ec2-user/SageMaker/tmp'}):\n",
    "    for file in input_files:\n",
    "        print(f'processing {file}...')\n",
    "        \n",
    "        START = time()\n",
    "        \n",
    "        print(\"- reading in parquet file\")\n",
    "        now = time()\n",
    "        region = file[4:-8]\n",
    "        df = pd.read_parquet(f\"{files_dir}/{file}\")\n",
    "        ddf = dd.from_pandas(df, npartitions=64)\n",
    "        print(f\"Took {time() - now:.2f} seconds\")\n",
    "\n",
    "\n",
    "#         print(\"- processing individual stats\")\n",
    "#         now = time()\n",
    "#         final_df = (ddf\n",
    "#                 .groupby('da')\n",
    "#                 .apply(parse_user_level_features, \n",
    "#                        meta={'da':'int64', 'feature': 'object', 'feature_num': 'int64', 'feature_desc': 'object', 'sex': 'object', 'count': 'float', 'pct': 'float'}))\n",
    "\n",
    "#         final_df = final_df.compute(scheduler='processes')\n",
    "#         final_df = final_df.reset_index(drop=True)\n",
    "#         final_df = final_df.assign(region=region)\n",
    "#         final_df.to_parquet(f\"{files_dir}/{region}.parquet\", index=False)\n",
    "#         del final_df\n",
    "#         print(f\"Took {time() - now:.2f} seconds\")\n",
    "        \n",
    "        print(\"- processing overall stats\")\n",
    "        now = time()\n",
    "        final_df2 = (ddf\n",
    "                .groupby('da')\n",
    "                .apply(parse_overall_features, \n",
    "                       meta={'da':'int64', 'feature': 'object', 'feature_num': 'int64', 'feature_desc': 'object', 'sex': 'object', 'count': 'float'}))\n",
    "\n",
    "        final_df2 = final_df2.compute(scheduler='processes')\n",
    "        final_df2 = final_df2.reset_index(drop=True)\n",
    "        final_df2 = final_df2.assign(region=region)\n",
    "        final_df2.to_parquet(f\"{files_dir}/{region}_overall.parquet\", index=False)\n",
    "        del final_df2\n",
    "        print(f\"Took {time() - now:.2f} seconds\")\n",
    "        \n",
    "        del df\n",
    "        print(f\"Done! {file} took {time() - START:.2f} seconds\")\n",
    "\n",
    "        # Remove temp files generated by Dask to free up space\n",
    "        shutil.rmtree('/home/ec2-user/SageMaker/tmp')\n",
    "        os.makedirs('/home/ec2-user/SageMaker/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import parsed files to HDFS (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker')\n",
    "import ClouderaSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"yarn\")\n",
    "    .appName(\"stats_can\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.rpc.message.maxSize\", \"1024\")\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "hc = HiveContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload individual stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'QUEBEC_overall.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overall'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[-15:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRAIRIES.parquet',\n",
       " 'BRITISH_COLUMBIA.parquet',\n",
       " 'ONTARIO.parquet',\n",
       " 'QUEBEC.parquet',\n",
       " 'TERRITORIES.parquet',\n",
       " 'ATLANTIC.parquet']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_files = [f for f in os.listdir(files_dir) if f.endswith('.parquet') and f[:3] != 'raw' and f[-15:-8] != 'overall' and f[:4] != 'prov']\n",
    "parsed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([StructField(\"da\", IntegerType(), True),\n",
    "                       StructField(\"feature\", StringType(), True),\n",
    "                       StructField(\"feature_num\", IntegerType(), True),\n",
    "                       StructField(\"feature_desc\", StringType(), True),\n",
    "                       StructField(\"sex\", StringType(), True),\n",
    "                       StructField(\"count\", FloatType(), True),\n",
    "                       StructField(\"pct\", FloatType(), True),\n",
    "                       StructField(\"region\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading prov_ONTARIO.parquet\n",
      "uploading prov_ATLANTIC.parquet\n",
      "uploading prov_TERRITORIES.parquet\n",
      "uploading prov_PRAIRIES.parquet\n",
      "uploading prov_QUEBEC.parquet\n",
      "uploading prov_BRITISH_COLUMBIA.parquet\n"
     ]
    }
   ],
   "source": [
    "hc.sql('DROP TABLE IF EXISTS hiveaeprodanuser.statscan_demographics_prov')\n",
    "\n",
    "for file in parsed_files:\n",
    "    print(f\"uploading {file}\")\n",
    "    df = pd.read_parquet(f\"{files_dir}/{file}\")\n",
    "    s_df = spark.createDataFrame(df, schema=mySchema)\n",
    "    s_df.write.mode('append').saveAsTable('hiveaeprodanuser.statscan_demographics_prov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QUEBEC_overall.parquet',\n",
       " 'TERRITORIES_overall.parquet',\n",
       " 'ONTARIO_overall.parquet',\n",
       " 'BRITISH_COLUMBIA_overall.parquet',\n",
       " 'ATLANTIC_overall.parquet',\n",
       " 'PRAIRIES_overall.parquet']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_parsed_files = [f for f in os.listdir(files_dir) if f.endswith('.parquet') and f[:3] != 'raw' and f[-15:-8] == 'overall' and f[:4] != 'prov']\n",
    "overall_parsed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([StructField(\"da\", IntegerType(), True),\n",
    "                       StructField(\"feature\", StringType(), True),\n",
    "                       StructField(\"feature_num\", IntegerType(), True),\n",
    "                       StructField(\"feature_desc\", StringType(), True),\n",
    "                       StructField(\"sex\", StringType(), True),\n",
    "                       StructField(\"count\", FloatType(), True),\n",
    "                       StructField(\"region\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql('DROP TABLE IF EXISTS hiveaeprodanuser.statscan_demographics_overall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading QUEBEC_overall.parquet\n",
      "uploading TERRITORIES_overall.parquet\n",
      "uploading ONTARIO_overall.parquet\n",
      "uploading BRITISH_COLUMBIA_overall.parquet\n",
      "uploading ATLANTIC_overall.parquet\n",
      "uploading PRAIRIES_overall.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in overall_parsed_files:\n",
    "    print(f\"uploading {file}\")\n",
    "    df = pd.read_parquet(f\"{files_dir}/{file}\")\n",
    "    s_df = spark.createDataFrame(df, schema=mySchema)\n",
    "    s_df.write.mode('append').saveAsTable('hiveaeprodanuser.statscan_demographics_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = hc.table('hiveaeprodanuser.statscan_demographics_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          feature|\n",
      "+-----------------+\n",
      "| household income|\n",
      "|  employment_rate|\n",
      "|   dwelling value|\n",
      "|individual income|\n",
      "|    overall stats|\n",
      "|              age|\n",
      "|    dwelling rent|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.select('feature').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pccf = hc.table('hiveaeprodref.stats_can_pccf')\n",
    "statscan = hc.table('hiveaeprodanuser.statscan_demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|          region|  count|\n",
      "+----------------+-------+\n",
      "|     TERRITORIES|  65315|\n",
      "|        ATLANTIC|1570275|\n",
      "|BRITISH_COLUMBIA|2595644|\n",
      "|        PRAIRIES|3547586|\n",
      "|          QUEBEC|4860946|\n",
      "|         ONTARIO|7198246|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statscan.groupby('region').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What dissemination areas are missing?\n",
    "pccf_da = pccf.selectExpr('cast(dissemination_area_uid as int) as da').distinct()\n",
    "sc_da = statscan.select('da').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47429 unique DAs in the PCCF table\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {pccf_da.count()} unique DAs in the PCCF table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55044 unique DAs in the parsed statscan demographics table\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {sc_da.count()} unique DAs in the parsed statscan demographics table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 147 DAs missing from the parsed demographics table that are in the PCCF table\n"
     ]
    }
   ],
   "source": [
    "# Number of dissemination areas that aren't in the parsed Census data\n",
    "missing_das = pccf_da.subtract(sc_da)\n",
    "print(f\"There are {missing_das.count()} DAs missing from the parsed demographics table that are in the PCCF table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This affects 2158 rows in the PCCF table.\n"
     ]
    }
   ],
   "source": [
    "affected_row_cnt = pccf.where(col('dissemination_area_uid').isin([x.da for x in missing_das.collect()])).count()\n",
    "print(f\"This affects {affected_row_cnt} rows in the PCCF table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862778"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pccf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|          region|  count|\n",
      "+----------------+-------+\n",
      "|     TERRITORIES|  65315|\n",
      "|        ATLANTIC|1570275|\n",
      "|BRITISH_COLUMBIA|2595644|\n",
      "|        PRAIRIES|3547586|\n",
      "|          QUEBEC|4860946|\n",
      "|         ONTARIO|7198246|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statscan.groupby('region').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_parquet(f\"{files_dir}/QUEBEC_overall.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['overall stats', 'age', 'individual income', 'household income',\n",
       "       'dwelling value', 'dwelling rent', 'employment_rate'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.feature.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3_persisted",
   "language": "python",
   "name": "conda_python3_persisted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
